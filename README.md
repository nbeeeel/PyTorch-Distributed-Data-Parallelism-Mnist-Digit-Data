# ğŸš€ Distributed MNIST Training with TensorFlow & PyTorch (MultiWorkerMirroredStrategy + SLURM)

This repository demonstrates **distributed deep learning** on the **MNIST digit classification task** using:

- **TensorFlowâ€™s `MultiWorkerMirroredStrategy`** (for MNIST dataset handling + demonstration)
- **PyTorch Distributed Data Parallel (DDP)** backend concepts (integration-ready)
- **NCCL backend for GPU communication**
- **SLURM job scheduler** for multi-node orchestration

The project is modularized for clarity, extensibility, and easy adaptation to other datasets or models.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ config/
â”‚   â””â”€â”€ env_setup.py       # Environment + TF_CONFIG setup
â”œâ”€â”€ data/
â”‚   â””â”€â”€ dataset.py         # MNIST dataset loader (download + preprocess)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ simple_cnn.py      # Simple CNN model for digit classification
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ strategy.py        # Distributed strategy initialization
â”‚   â””â”€â”€ trainer.py         # Training loop (with gradient updates)
â”œâ”€â”€ train.py               # Entry point (orchestrates everything)
â”œâ”€â”€ run_slurm.sh           # SLURM job script for multi-node execution
â””â”€â”€ README.md              # Project documentation
```

---

## âœ¨ Features

- âœ… Multi-node **data parallel training**
- âœ… **NCCL communication backend** for GPU communication
- âœ… Modularized codebase (datasets/models/training loop)
- âœ… SLURM job script for running across a **GPU cluster**
- âœ… Barrier sync + retry logic for robustness
- âœ… Uses **MNIST dataset** (auto-downloaded)

---

## âš™ï¸ Installation

Clone this repository:

```bash
git clone https://github.com/nbeeeel/PyTorch-Distributed-Data-Parallelism-Mnist-Digit-Data.git
cd PyTorch-Distributed-Data-Parallelism-Mnist-Digit-Data
pip install -r requirements.txt
```

### Requirements

- **Python 3.8+**
- **TensorFlow 2.11+**
- **PyTorch 1.12+**
- **CUDA 11.8 (or matching cluster version)**
- **NCCL 2.12+**

---

## ğŸ“Š Model Architecture

The **SimpleCNN** defined in `models/simple_cnn.py`:

- Flatten input (28Ã—28Ã—1)
- Dense layer (128 units, ReLU)
- Dense layer (10 units, logits for classification)

---

## ğŸ§© How It Works

### ğŸ”¹ Step 1. Environment Setup
Each worker configures:
- `TF_CONFIG` with cluster topology
- NCCL communication interface
- GRPC keepalive options

### ğŸ”¹ Step 2. Strategy Initialization
- Uses **`MultiWorkerMirroredStrategy`** with **NCCL backend**
- Retries initialization on failure (up to 3 times)

### ğŸ”¹ Step 3. Dataset Loading
- Downloads and preprocesses **MNIST**
- Normalizes to `[0,1]`
- Builds shuffled + batched `tf.data.Dataset`

### ğŸ”¹ Step 4. Training Loop
- Each replica runs `train_step` with gradient tape
- Loss is reduced across replicas
- Leader rank (`rank=0`) logs epoch loss

---

## ğŸ–¥ï¸ Running with SLURM

A sample SLURM job script (`run_slurm.sh`) is included:

```bash
#!/bin/bash
#SBATCH --job-name=nccl_dist
#SBATCH --nodes=5
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=4
#SBATCH --gpus-per-node=1
#SBATCH --nodelist=node01,node02,node03,node04,node05
#SBATCH --time=01:00:00
#SBATCH --output=nccl_%j.out
#SBATCH --error=nccl_%j.err

# Master node address and port
export MASTER_ADDR=192.168.20.15
export MASTER_PORT=29500

export MPLCONFIGDIR=/tmp/matplotlib-$SLURM_JOBID

# Example: module load cuda/11.8 nccl/2.12
# module load cuda/11.8

srun --mpi=none bash -c '
RANK=$SLURM_PROCID
WORLD_SIZE=$SLURM_NTASKS

case $RANK in
  0) export NCCL_SOCKET_IFNAME=ens1f1 ;;
  1) export NCCL_SOCKET_IFNAME=ens1f1 ;;
  2) export NCCL_SOCKET_IFNAME=ens1f0np0 ;;
  3) export NCCL_SOCKET_IFNAME=ens1f1 ;;
  4) export NCCL_SOCKET_IFNAME=ens1f0 ;;
  5) export NCCL_SOCKET_IFNAME=ens1f0np0 ;;
esac

export NCCL_IB_DISABLE=1
export NCCL_DEBUG=INFO
export PYTHONUNBUFFERED=1

export RANK
export WORLD_SIZE
export MASTER_ADDR
export MASTER_PORT
export MPLCONFIGDIR

python3 ./train.py
'
```

### âœ… Submit job:

```bash
sbatch run_slurm.sh
```

Logs are saved as `nccl_<JOBID>.out` and `nccl_<JOBID>.err`.

---

## ğŸ“ˆ Example Output

```text
RANK 0 on node01: Setting TF_CONFIG: {...}
RANK 1 on node02: Connected to 192.168.20.15:29500
[Epoch 0] Loss: 0.2751
[Epoch 1] Loss: 0.1927
...
```

---

## ğŸ“Œ Notes & Tips

- Ensure **all nodes have identical environments** (same TF/PyTorch/CUDA/NCCL versions).
- Adjust **`NCCL_SOCKET_IFNAME`** to match your clusterâ€™s network interfaces.
- Use `NCCL_DEBUG=INFO` for diagnostics.
- Modify **`train.py`** to plug in new datasets or models.

---

## ğŸ”® Future Extensions

- Add checkpoint saving and resume support
- Extend to CIFAR-10 / ImageNet
- Replace SimpleCNN with deeper models (ResNet, EfficientNet)
- Add evaluation pipeline

---

## ğŸ“œ License

This project is released under the **MIT License**.

---

## ğŸ¤ Contributing

Contributions are welcome!  
Open an **issue** or **pull request** with improvements or bug fixes.

---
